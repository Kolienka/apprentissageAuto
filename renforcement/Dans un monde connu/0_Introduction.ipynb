{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prise en main de l'environnement\n",
    "\n",
    "Dans ce notebook nous allons découvrir les différents aspects de l'environnement.\n",
    "En particulier, nous allons nous familiariser avec OpenAI Gym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gérer les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "from bucket_env import BucketEnv3\n",
    "from bucket_env import rendering\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialiser l'environnement\n",
    "Pour créer l'environnement, il faut simplement créer une instance de `BucketEnv3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### env.reset()\n",
    "Cette méthode positionne l'environnement dans son état initial et le renvoie de manière à ce que l'agent puisse l'observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "initial_state = env.reset()\n",
    "print(f\"Le nouvel épisode commence dans l'état: \\n{initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.render()\n",
    "Cette méthode génère une image qui représente l'état courant de l'environnement, sous la forme d'un np.darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(\"Etat initial\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of states: {env.observation_space}\")\n",
    "print(f\"Number of actions: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.step()\n",
    "Cette méthode applique une action choisie par l'agent dans l'environnement, pour le modifier. En réponse, l'environnement renvoie un tuple de quatre objets:\n",
    "- l'état suivant,\n",
    "- la récompense obtenue,\n",
    "- (bool) si la tâche est accomplie\n",
    "- n'importe quelles autres informations pertinentes dans un dictionnaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "action = np.random.choice(5)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"Aprés avoir placé un colis avec l'action {action}, l'environnement est dans l'état:\\n {next_state}\")\n",
    "print(f\"La récompense obtenue est: {reward}\")\n",
    "print(\"La tâche\", \"est\" if done else \"n'est pas\", \"terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Afficher le nouvel état"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nouvel état\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.close()\n",
    "Cela termine la tâche et ferme l'environnement, libérant les ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## L'environnement Bucket : placer des colis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans cette section, nous allons nous familiariser avec l'environnement que nous allons utiliser. Cet environnement est pratique pour apprendre les bases de l'apprentissage par renforcement parce que:\n",
    "- il offre peu d'actions (5)\n",
    "- les transitions entre les états sont déterministes ($p(s', r| s, a) = 1$)\n",
    "- les récompenses sont proportionnelles au nombre de colis placés\n",
    "- l'espace des états est relativement faible ($7^3 = 343$)\n",
    "\n",
    "Nous allons pouvoir utiliser les concepts suivants:\n",
    "- états et espace d'états\n",
    "- actions et espace d'actions\n",
    "- trajectoires et épisodes\n",
    "- récompenses et gains\n",
    "- politique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cet environnement représente un carton de 3 x 5 (l x h), dans lequel le but de l'agent est de placer le plus de colis possible. \n",
    "\n",
    "Tous les colis sont identiques: un rectangle de dimension (l x h): 1 x 2.\n",
    "L'orientation d'un colis se fait par rapport à un cube de référence.\n",
    "Ce colis dispose de deux orientations possibles:\n",
    "- 0 : (l x h): 1 x 2, le cube de référence est en (0,0),\n",
    "- 1: (l x h): 2 x 1, le cube de référence est en (0,0).\n",
    "\n",
    "L'objectif est de placer 7 colis dans le carton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Création de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Etats et espace des états\n",
    "Le fond du carton est discrétisé en 15 cellules, pouvant prendre des valeurs dans l'intervalle [0,1]. La valeur d'une cellule représente son occupation par un colis.\n",
    "\n",
    "Un état est un tuple de 3 valeurs comprises dans l'intervalle [0,4].\n",
    "\n",
    "L'espace d'états est faible : $7^{3} = 343$, et en pratique, certains états ne sont jamais atteignables.\n",
    "En effet, il est considéré que les colis ne *volent* pas et repose forcément sur le fond du carton ou un autre colis (même partiellement). \n",
    "\n",
    "Les informations relatives à l'espace des états sont stockées dans env.observation_space qui est une instance de MultiDiscrete([7 7 7]). Cela indique qu'elle est composé de 3 éléments (les colonnes), chacun pouvant prendre 7 valeurs différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"L'état initial est: {env.reset()}\")\n",
    "print(f\"L'espace des états est de type: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Actions et espace des actions\n",
    "Dans cet environnement, il y a 5 actions différentes, elle sont représentées par un entier:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\in \\{0, 1, 2, 3, 4\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 -> colis dans l'orientation 0 avec x = 0\n",
    "- 1 -> colis dans l'orientation 0 avec x = 1\n",
    "- 2 -> colis dans l'orientation 0 avec x = 2\n",
    "- 3 -> colis dans l'orientation 1 avec x = 0\n",
    "- 4 -> colis dans l'orientation 1 avec x = 1\n",
    "\n",
    "Pour éxécuter une action, il suffit de passer l'entier correspondant à la méthode `env.step`.\n",
    "\n",
    "Les informations relatives à l'espace des actions sont stockées dans env.action_space qui est une instance de Discrete(5). Cela indique qu'elle définit l'intervalle [0,4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Un exemple d'action valide est: {env.action_space.sample()}\")\n",
    "print(f\"L'espace des actions est de type: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Trajectoires et épisodes\n",
    "Une trajectoire est une séquence générée en passant d'un état à un autre:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n",
    "\\end{equation}\n",
    "\n",
    "Par exemple, voici une trajectoire de 3 actions prises aléatoirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()\n",
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Un exemple de trajectoire:\\n{trajectory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un épisode est une trajectoire qui part de l'état initial et atteint un état final:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n",
    "\\end{equation}\n",
    "où T est un état final.\n",
    "\n",
    "Par exemple, voici un épisode entier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()\n",
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Un exemple d'épisode:\\n{episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Récompenses et gains\n",
    "Une récompense est un valeur numérique renvoyé par l'environnement apès l'application d'une action *a* prise par l'agent dans l'état *s*:\n",
    "\n",
    "\\begin{equation}\n",
    "    r = r(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Par exemple, voici une récompense obtenue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _ = env.step(action)\n",
    "print(f\"Nous avons obtenu une récompense de {reward} en prenant l'action {action} depuis l'état {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le gain associé au temps *t* est la somme (pondérées) des récompenses que l'agent a obtenu jusqu'à ce moment.\n",
    "Il est possible de calculer $G_0$, c'est-à-dire le gain au début de l'épisode:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "En considérant un facteur de *rabais* $\\gamma = 0.99$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()\n",
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.99\n",
    "G_0 = 0\n",
    "t = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()\n",
    "\n",
    "print(\n",
    "    f\"\"\"{t} colis ont été placés, le gain ainsi obtenu est de {G_0}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Politique\n",
    "\n",
    "Une politique est une fonction $\\pi(a|s) \\in [0, 1]$ qui donne la probabilité d'une action depuis l'état courant.\n",
    "Cette fonction prend en argument un état et une action et retourne une valeur dans [0.,1.].\n",
    "\n",
    "En pratique, nous avons de calculer la probabilités de toutes les actions, nous représenterons une politique comme une fonction qui prend un état en argumene et renvoie les probabilités associées à chaque action.\n",
    "Donc, si les probabilités sont:\n",
    "    \n",
    "[0.5, 0.3, 0.2]\n",
    "\n",
    "l'action à l'indice 0 a 50% de probabiltés d'être choisie, celle à l'indice 1 est à 30% et la dernière à 20%.\n",
    "\n",
    "Nous pouvons définir une politique qui choisit les actions aléatoirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([1/5] * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appliquons cette politique sur un épisode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Créer et ré-initialiser l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Calculer $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3, 4\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "action_probabilities = random_policy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "objects = tuple([i for i in range(5)])\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, action_probabilities, alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('P(a|s)')\n",
    "plt.title('Politique aléatoire')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Utilisons cette politique au cours d'un épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rendering(env, random_policy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
