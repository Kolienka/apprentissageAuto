{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prise en main de l'environnement\n",
    "\n",
    "Dans ce notebook nous allons découvrir les différents aspects de l'environnement.\n",
    "En particulier, nous allons nous familiariser avec OpenAI Gym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gérer les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "from bucket_env import BucketEnv3\n",
    "from bucket_env import rendering\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialiser l'environnement\n",
    "Pour créer l'environnement, il faut simplement créer une instance de `BucketEnv3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### env.reset()\n",
    "Cette méthode positionne l'environnement dans son état initial et le renvoie de manière à ce que l'agent puisse l'observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.render()\n",
    "Cette méthode génère une image qui représente l'état courant de l'environnement, sous la forme d'un `np.darray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayEnv(env):\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    plt.axis('off')\n",
    "    plt.title('Env')\n",
    "    plt.imshow(frame)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Ce dossier contient les éléments nécessaires à la réalisation du projet d'apprentissage par renforcement de l'UE AAA.\n",
    "\n",
    "Déroulé du cours\n",
    "Ce module se déroule en deux temps:\n",
    "\n",
    "La première séance est consacrée aux apports théoriques de l'apprentissage par renforcement ainsi que la prise en main de l'environnement d'étude,\n",
    "La seconde séance sera consacrée à l'implémentation d'algorithmes de RL.\n",
    "Préparation de l'environnement de travail\n",
    "Votre travail se fera sur des Notebooks Jupyter, en utilisant différentes packages Python.\n",
    "\n",
    "Il va donc être nécessaire d'avoir une version récente de Python et de pip, le gestionnaire de paquet.\n",
    "\n",
    "Vous pouvez suivre les instructions suivantes pour installer l'un ou l'autre:\n",
    "\n",
    "Installer la dernière version de Python \n",
    "Installer pip\n",
    "Une fois Python et pip installés, vous devez executer la ligne suivante dans un terminal:\n",
    "\n",
    "    pip install matplotlib numpy tqdm jupyter gym==0.21.0\n",
    "\n",
    "Ensuite, vous devez télécharger l'archive nommée ProjetRL2022.zip et la décompresser dans le dossier de votre choix.\n",
    "\n",
    "Puis, ouvrez un terminal et rendez vous dans le dossier nouvellement créé et lancez la commande suivante:\n",
    "\n",
    "    jupyter notebook\n",
    "\n",
    "Une nouvelle fenêtre de votre navigateur s'ouvre affichant le contenu du dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.step()\n",
    "Cette méthode applique une action choisie par l'agent dans l'environnement, pour le modifier. En réponse, l'environnement renvoie un tuple de quatre objets:\n",
    "- l'état suivant,\n",
    "- la récompense obtenue,\n",
    "- (bool) si la tâche est accomplie\n",
    "- n'importe quelles autres informations pertinentes dans un dictionnaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 0, 0), 1.0, False, {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "action = np.random.choice(5)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Afficher le nouvel état"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12aa6d690>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.render()\n",
    "plt.axis('off')\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### env.close()\n",
    "Cela termine la tâche et ferme l'environnement, libérant les ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## L'environnement Bucket : placer des colis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans cette section, nous allons nous familiariser avec l'environnement que nous allons utiliser. Cet environnement est pratique pour apprendre les bases de l'apprentissage par renforcement parce que:\n",
    "- il offre peu d'actions (5)\n",
    "- les transitions entre les états sont déterministes ($p(s', r| s, a) = 1$)\n",
    "- les récompenses sont proportionnelles au nombre de colis placés\n",
    "- l'espace des états est relativement faible ($7^3 = 343$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nous allons pouvoir utiliser les concepts suivants:\n",
    "- états et espace d'états\n",
    "- actions et espace d'actions\n",
    "- trajectoires et épisodes\n",
    "- récompenses et gains\n",
    "- politique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cet environnement représente un carton de 3 x 5 (l x h), dans lequel le but de l'agent est de placer le plus de colis possible. \n",
    "\n",
    "Tous les colis sont identiques: un rectangle de dimension (l x h): 1 x 2.\n",
    "L'orientation d'un colis se fait par rapport à un cube de référence.\n",
    "Ce colis dispose de deux orientations possibles:\n",
    "- 0 : (l x h): 1 x 2, le cube de référence est en (0,0),\n",
    "- 1: (l x h): 2 x 1, le cube de référence est en (0,0).\n",
    "\n",
    "L'objectif est de placer 7 colis dans le carton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Création de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = BucketEnv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Etats et espace des états\n",
    "Le fond du carton est discrétisé en 15 cellules, pouvant prendre des valeurs dans l'intervalle [0,1]. La valeur d'une cellule représente son occupation par un colis.\n",
    "\n",
    "Un état est un tuple de 3 valeurs comprises dans l'intervalle [0,6] (mais seules les valeurs < 5 sont intéressantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'espace d'états est faible : $7^{3} = 343$, et en pratique, certains états ne sont jamais atteignables.\n",
    "En effet, il est considéré que les colis ne *volent* pas et repose forcément sur le fond du carton ou un autre colis (même partiellement). \n",
    "\n",
    "Les informations relatives à l'espace des états sont stockées dans `env.observation_space` qui est une instance de `MultiDiscrete([7 7 7])`. Cela indique qu'elle est composé de 3 éléments (les colonnes), chacun pouvant prendre 7 valeurs différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([7 7 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Actions et espace des actions\n",
    "Dans cet environnement, il y a 5 actions différentes, elle sont représentées par un entier:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\in \\{0, 1, 2, 3, 4\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 -> colis dans l'orientation 0 avec x = 0\n",
    "- 1 -> colis dans l'orientation 0 avec x = 1\n",
    "- 2 -> colis dans l'orientation 0 avec x = 2\n",
    "- 3 -> colis dans l'orientation 1 avec x = 0\n",
    "- 4 -> colis dans l'orientation 1 avec x = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pour éxécuter une action, il suffit de passer l'entier correspondant à la méthode `env.step`.\n",
    "\n",
    "Les informations relatives à l'espace des actions sont stockées dans env.action_space qui est une instance de Discrete(5). Cela indique qu'elle définit l'intervalle [0,4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Trajectoires et épisodes\n",
    "Une trajectoire est une séquence générée en passant d'un état à un autre:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n",
    "\\end{equation}\n",
    "\n",
    "Par exemple, voici une trajectoire de 3 actions prises aléatoirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La trajectoire est : [[(0, 0, 0), 1, 1.0, (0, 2, 0)], [(0, 2, 0), 1, 1.0, (0, 4, 0)], [(0, 4, 0), 2, 1.0, (0, 4, 2)]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = np.random.choice(5)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    trajectory.append([state,action,reward,next_state])\n",
    "    state = next_state\n",
    "print(f'La trajectoire est : {trajectory}')\n",
    "displayEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un épisode est une trajectoire qui part de l'état initial et atteint un état final:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n",
    "\\end{equation}\n",
    "où T est un état final.\n",
    "\n",
    "Par exemple, voici un épisode entier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 0), 2, 1.0, (0, 0, 2)], [(0, 0, 2), 1, 1.0, (0, 2, 2)], [(0, 2, 2), 0, 1.0, (2, 2, 2)], [(2, 2, 2), 0, 1.0, (4, 2, 2)], [(4, 2, 2), 1, 1.0, (4, 4, 2)], [(4, 4, 2), 1, 0.0, (4, 5, 2)]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice(5)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode.append([state,action,reward,next_state])\n",
    "    state = next_state\n",
    "print(episode)\n",
    "displayEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Récompenses et gains\n",
    "Une récompense est un valeur numérique renvoyé par l'environnement apès l'application d'une action *a* prise par l'agent dans l'état *s*:\n",
    "\n",
    "\\begin{equation}\n",
    "    r = r(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Par exemple, voici une récompense obtenue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "_, reward, _, _ = env.step(1)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le gain associé au temps *t* est la somme (pondérées) des récompenses que l'agent a obtenu jusqu'à ce moment.\n",
    "Il est possible de calculer $G_0$, c'est-à-dire le gain au début de l'épisode:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "En considérant un facteur de *rabais* $\\gamma = 0.99$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le gain obtenu est 3.940399\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "G = 0\n",
    "gamma = 0.99\n",
    "done = False\n",
    "t = 0 \n",
    "\n",
    "while not done:\n",
    "    action = np.random.choice(5)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    G += reward * gamma ** t\n",
    "    t += 1\n",
    "print(f'Le gain obtenu est {G}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Politique\n",
    "\n",
    "Une politique est une fonction $\\pi(a|s) \\in [0, 1]$ qui donne la probabilité d'une action depuis l'état courant.\n",
    "Cette fonction prend en argument un état et une action et retourne une valeur dans [0.,1.].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En pratique, nous avons de calculer la probabilités de toutes les actions, nous représenterons une politique comme une fonction qui prend un état en argumene et renvoie les probabilités associées à chaque action.\n",
    "Donc, si les probabilités sont:\n",
    "    \n",
    "[0.5, 0.3, 0.2]\n",
    "\n",
    "l'action à l'indice 0 a 50% de probabiltés d'être choisie, celle à l'indice 1 est à 30% et la dernière à 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Nous pouvons définir une politique qui choisit les actions aléatoirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([1/5] * 5)\n",
    "print(random_policy(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appliquons cette politique sur un épisode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Créer et ré-initialiser l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BucketEnv3()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Calculer $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3, 4\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Utilisons cette politique au cours d'un épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
